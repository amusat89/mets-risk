{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "82a42c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7cf15f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "229ab026",
   "metadata": {},
   "outputs": [],
   "source": [
    "metsadult = pd.read_csv('/Users/saheed/Desktop/My_Rprog_Journey/metab_tensorflow.csv')\n",
    "metsadult.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ad5a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying your current encoding:\n",
      "  metabolic_syndrome  target\n",
      "0            Absence       0\n",
      "1            Absence       0\n",
      "2            Absence       0\n",
      "3           Presence       1\n",
      "4           Presence       1\n"
     ]
    }
   ],
   "source": [
    "# ACSVD risk factor encoding: Absence = 0, Presence = 1\n",
    "\n",
    "metsadult['target'] = np.where(metsadult['metabolic_syndrome']== 'Presence', 1, 0)\n",
    "\n",
    "print(\"Verifying your current encoding:\")\n",
    "sample_check = metsadult[['metabolic_syndrome', 'target']].head()\n",
    "print(sample_check)\n",
    "\n",
    "# Drop unused features.\n",
    "dataframe = metsadult.drop(columns=['metabolic_syndrome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eeb0db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deselect non-predictive features.\n",
    "X = dataframe.drop(columns=['target', 'hypercholesterolaemia', 'Gender', 'BMI'])  # Features\n",
    "y = dataframe['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4541e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1710, 22)\n",
      "   LBDAPBSI  BMXBMI  BMXWAIST  Systolic_BP  Diastolic_BP  RIDAGEYR  RIAGENDR  \\\n",
      "0      1.29    30.8     107.9   140.000000     86.000000        53         1   \n",
      "1      0.72    28.0      86.6   111.333333     72.666667        22         1   \n",
      "2      0.92    24.1      90.1   110.666667     72.000000        45         1   \n",
      "3      1.24    35.4     113.5   142.666667     62.666667        57         2   \n",
      "4      1.29    25.3      79.5   107.333333     61.333333        24         2   \n",
      "\n",
      "   DXXSATA  DXXSATM  DXXVFATA  ...  LBDGLUSI  LBDHDDSI  LBXHSCRP  LBDINSI  \\\n",
      "0   260.01  1253.60    200.60  ...      5.59      1.63       1.4   103.56   \n",
      "1   264.63  1275.84     65.87  ...      5.27      1.24       1.3    68.34   \n",
      "2   162.77   784.75     67.03  ...      4.68      1.29       0.3    17.16   \n",
      "3   513.64  2476.40    209.81  ...     22.10      1.11       3.9    33.42   \n",
      "4   300.20  1447.35     80.53  ...      5.27      1.06       1.7    79.38   \n",
      "\n",
      "   LBDTCSI  LBDTRSI  LBDLDLSI  eLDL_Trig  Fasting_hrs  target  \n",
      "0     6.85    1.660     4.474   0.499772    12.033333       0  \n",
      "1     4.24    0.768     2.638   0.302829     9.850000       0  \n",
      "2     4.68    0.587     3.129   0.304001    11.000000       0  \n",
      "3     5.87    1.547     4.060   0.465069     9.750000       1  \n",
      "4     6.28    2.269     4.189   0.545093    10.600000       1  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "df_smote = pd.concat([pd.DataFrame(X_res, columns=X.columns),\n",
    "                      pd.Series(y_res, name='target')], axis=1)\n",
    "df_smote['target'].value_counts()\n",
    "print(df_smote.shape)\n",
    "print(df_smote.head())\n",
    "dataframe_smoted = df_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dbc214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saheed/my-tensorflow/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "## Split the data into training, validation, and test sets (80% train, 10% val, 10% test)\n",
    "train, val, test = np.split(dataframe_smoted.sample(frac=1), [int(0.8*len(dataframe_smoted)), int(0.9*len(dataframe_smoted))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40ab59b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368 training samples\n",
      "171 validation samples\n",
      "171 test samples\n"
     ]
    }
   ],
   "source": [
    "# Verify the sizes of each set\n",
    "print(len(train), 'training samples')\n",
    "print(len(val), 'validation samples')\n",
    "print(len(test), 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4a178454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets; batch size of 32, shuffle the training data, and prefetch for performance\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  df = dataframe_smoted.copy()\n",
    "  labels = df.pop('target')\n",
    "  df = {key: value.to_numpy()[:,tf.newaxis] for key, value in dataframe_smoted.items()}\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ab741d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset, \n",
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "08eb0d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=({'LBDAPBSI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'BMXBMI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'BMXWAIST': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'Systolic_BP': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'Diastolic_BP': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'RIDAGEYR': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'RIAGENDR': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'DXXSATA': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'DXXSATM': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'DXXVFATA': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'DXXVFATM': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBXGH': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBDGLUSI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBDHDDSI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBXHSCRP': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBDINSI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBDTCSI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBDTRSI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'LBDLDLSI': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'eLDL_Trig': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'Fasting_hrs': TensorSpec(shape=(None, 1), dtype=tf.float64, name=None), 'target': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a batch of the training dataset\n",
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0185b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['LBDAPBSI', 'BMXBMI', 'BMXWAIST', 'Systolic_BP', 'Diastolic_BP', 'RIDAGEYR', 'RIAGENDR', 'DXXSATA', 'DXXSATM', 'DXXVFATA', 'DXXVFATM', 'LBXGH', 'LBDGLUSI', 'LBDHDDSI', 'LBXHSCRP', 'LBDINSI', 'LBDTCSI', 'LBDTRSI', 'LBDLDLSI', 'eLDL_Trig', 'Fasting_hrs', 'target']\n"
     ]
    }
   ],
   "source": [
    "# Display the features contained in the batch\n",
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4fd3628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A batch of targets: tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Display the first feature and label batch\n",
    "print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99a4084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features such as Age, Systolic_BP, Diastolic_BP, Fasting_Blood_Sugar, Triglycerides, HDL_Cholesterol, Waist_Circumference. \n",
    "# Because these features have different ranges, normalizing them helps the model learn more effectively.\n",
    "def get_normalization_layer(name, dataset):\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    normalizer.adapt(feature_ds)\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d4c666f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric features to be normalized\n",
    "numeric_features = ['LBDAPBSI', 'BMXBMI', 'BMXWAIST', 'Systolic_BP', 'Diastolic_BP', \n",
    "                   'RIDAGEYR', 'RIAGENDR', 'DXXSATA', 'DXXSATM', 'DXXVFATA', \n",
    "                   'DXXVFATM', 'LBXGH', 'LBDGLUSI', 'LBDHDDSI', 'LBXHSCRP', \n",
    "                   'LBDINSI', 'LBDTCSI', 'LBDTRSI', 'LBDLDLSI', 'eLDL_Trig', \n",
    "                   'Fasting_hrs']\n",
    "\n",
    "# Create input layers and normalization layers for each numeric feature\n",
    "all_inputs = {}\n",
    "encoded_features = []\n",
    "\n",
    "# then for each numeric feature, create an input layer and a normalization layer\n",
    "for header in numeric_features:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs[header] = numeric_col\n",
    "    encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7594869d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[ 0.07841249],\n",
       "       [-0.7357946 ],\n",
       "       [ 0.26090738],\n",
       "       [-0.66560435],\n",
       "       [-0.83406115]], dtype=float32)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example usage of the normalization layer\n",
    "bmi_count_col = train_features['BMXBMI']\n",
    "layer = get_normalization_layer('BMXBMI', train_ds)\n",
    "layer(bmi_count_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "441dc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a category encoding layer for categorical features\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d087db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the validation and test datasets.\n",
    "batch_size = 256\n",
    "\n",
    "# Create the training dataset, shuffle the training data to ensure randomness during training and \n",
    "# ensure performance with prefetching and batching to the specified batch size\n",
    "\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "\n",
    "# shuffle to ensure randomness during training but not during validation and testing\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "736e1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create input layers and normalization layers for each numeric feature\n",
    "all_inputs = {}\n",
    "\n",
    "encoded_features = []\n",
    "\n",
    "# then for each numeric feature, create an input layer and a normalization layer\n",
    "for header in ['LBDAPBSI', 'BMXBMI', 'BMXWAIST', 'Systolic_BP', 'Diastolic_BP', 'RIDAGEYR', 'RIAGENDR', 'DXXSATA',\n",
    "       'DXXSATM', 'DXXVFATA', 'DXXVFATM', 'LBXGH', 'LBDGLUSI', 'LBDHDDSI', 'LBXHSCRP', 'LBDINSI', 'LBDTCSI',\n",
    "       'LBDTRSI', 'LBDLDLSI', 'eLDL_Trig', 'Fasting_hrs']:\n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs[header] = numeric_col\n",
    "  encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e3775382",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the model using the Functional API\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0eee1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer for binary classification, using binary cross-entropy loss  for logits,\n",
    "# and tracking accuracy and AUC as metrics.\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.AUC(name='auc')],\n",
    "              run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d26fcaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "# in this case, we'll train for 100 epochs and use early stopping to prevent overfitting.\n",
    "# We'll monitor the validation loss and stop training if it doesn't improve for 10 consecutive epochs.\n",
    "# the parameters below include model.fit parameters such as epochs, validation data, and callbacks for early stopping.\n",
    "import pydot \n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbfa906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/7\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6233 - auc: 0.6007 - loss: 0.6987 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saheed/my-tensorflow/.venv/lib/python3.11/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: {'LBDAPBSI': 'LBDAPBSI', 'BMXBMI': 'BMXBMI', 'BMXWAIST': 'BMXWAIST', 'Systolic_BP': 'Systolic_BP', 'Diastolic_BP': 'Diastolic_BP', 'RIDAGEYR': 'RIDAGEYR', 'RIAGENDR': 'RIAGENDR', 'DXXSATA': 'DXXSATA', 'DXXSATM': 'DXXSATM', 'DXXVFATA': 'DXXVFATA', 'DXXVFATM': 'DXXVFATM', 'LBXGH': 'LBXGH', 'LBDGLUSI': 'LBDGLUSI', 'LBDHDDSI': 'LBDHDDSI', 'LBXHSCRP': 'LBXHSCRP', 'LBDINSI': 'LBDINSI', 'LBDTCSI': 'LBDTCSI', 'LBDTRSI': 'LBDTRSI', 'LBDLDLSI': 'LBDLDLSI', 'eLDL_Trig': 'eLDL_Trig', 'Fasting_hrs': 'Fasting_hrs'}\n",
      "Received: inputs={'LBDAPBSI': 'Tensor(shape=(256, 1))', 'BMXBMI': 'Tensor(shape=(256, 1))', 'BMXWAIST': 'Tensor(shape=(256, 1))', 'Systolic_BP': 'Tensor(shape=(256, 1))', 'Diastolic_BP': 'Tensor(shape=(256, 1))', 'RIDAGEYR': 'Tensor(shape=(256, 1))', 'RIAGENDR': 'Tensor(shape=(256, 1))', 'DXXSATA': 'Tensor(shape=(256, 1))', 'DXXSATM': 'Tensor(shape=(256, 1))', 'DXXVFATA': 'Tensor(shape=(256, 1))', 'DXXVFATM': 'Tensor(shape=(256, 1))', 'LBXGH': 'Tensor(shape=(256, 1))', 'LBDGLUSI': 'Tensor(shape=(256, 1))', 'LBDHDDSI': 'Tensor(shape=(256, 1))', 'LBXHSCRP': 'Tensor(shape=(256, 1))', 'LBDINSI': 'Tensor(shape=(256, 1))', 'LBDTCSI': 'Tensor(shape=(256, 1))', 'LBDTRSI': 'Tensor(shape=(256, 1))', 'LBDLDLSI': 'Tensor(shape=(256, 1))', 'eLDL_Trig': 'Tensor(shape=(256, 1))', 'Fasting_hrs': 'Tensor(shape=(256, 1))', 'target': 'Tensor(shape=(256, 1))'}\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5988 - auc: 0.6249 - loss: 0.7002 - val_accuracy: 0.6462 - val_auc: 0.7212 - val_loss: 0.5742\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saheed/my-tensorflow/.venv/lib/python3.11/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: {'LBDAPBSI': 'LBDAPBSI', 'BMXBMI': 'BMXBMI', 'BMXWAIST': 'BMXWAIST', 'Systolic_BP': 'Systolic_BP', 'Diastolic_BP': 'Diastolic_BP', 'RIDAGEYR': 'RIDAGEYR', 'RIAGENDR': 'RIAGENDR', 'DXXSATA': 'DXXSATA', 'DXXSATM': 'DXXSATM', 'DXXVFATA': 'DXXVFATA', 'DXXVFATM': 'DXXVFATM', 'LBXGH': 'LBXGH', 'LBDGLUSI': 'LBDGLUSI', 'LBDHDDSI': 'LBDHDDSI', 'LBXHSCRP': 'LBXHSCRP', 'LBDINSI': 'LBDINSI', 'LBDTCSI': 'LBDTCSI', 'LBDTRSI': 'LBDTRSI', 'LBDLDLSI': 'LBDLDLSI', 'eLDL_Trig': 'eLDL_Trig', 'Fasting_hrs': 'Fasting_hrs'}\n",
      "Received: inputs={'LBDAPBSI': 'Tensor(shape=(174, 1))', 'BMXBMI': 'Tensor(shape=(174, 1))', 'BMXWAIST': 'Tensor(shape=(174, 1))', 'Systolic_BP': 'Tensor(shape=(174, 1))', 'Diastolic_BP': 'Tensor(shape=(174, 1))', 'RIDAGEYR': 'Tensor(shape=(174, 1))', 'RIAGENDR': 'Tensor(shape=(174, 1))', 'DXXSATA': 'Tensor(shape=(174, 1))', 'DXXSATM': 'Tensor(shape=(174, 1))', 'DXXVFATA': 'Tensor(shape=(174, 1))', 'DXXVFATM': 'Tensor(shape=(174, 1))', 'LBXGH': 'Tensor(shape=(174, 1))', 'LBDGLUSI': 'Tensor(shape=(174, 1))', 'LBDHDDSI': 'Tensor(shape=(174, 1))', 'LBXHSCRP': 'Tensor(shape=(174, 1))', 'LBDINSI': 'Tensor(shape=(174, 1))', 'LBDTCSI': 'Tensor(shape=(174, 1))', 'LBDTRSI': 'Tensor(shape=(174, 1))', 'LBDLDLSI': 'Tensor(shape=(174, 1))', 'eLDL_Trig': 'Tensor(shape=(174, 1))', 'Fasting_hrs': 'Tensor(shape=(174, 1))', 'target': 'Tensor(shape=(174, 1))'}\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6275 - auc: 0.6713 - loss: 0.6423 - val_accuracy: 0.6842 - val_auc: 0.7688 - val_loss: 0.5334\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6696 - auc: 0.7276 - loss: 0.5857 - val_accuracy: 0.7158 - val_auc: 0.7962 - val_loss: 0.5043\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6982 - auc: 0.7480 - loss: 0.5525 - val_accuracy: 0.7368 - val_auc: 0.8062 - val_loss: 0.4830\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7099 - auc: 0.7536 - loss: 0.5500 - val_accuracy: 0.7579 - val_auc: 0.8184 - val_loss: 0.4661\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7339 - auc: 0.7747 - loss: 0.5360 - val_accuracy: 0.7725 - val_auc: 0.8268 - val_loss: 0.4518\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7281 - auc: 0.7768 - loss: 0.5181 - val_accuracy: 0.7801 - val_auc: 0.8378 - val_loss: 0.4392\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7526 - auc: 0.8026 - loss: 0.4741 - val_accuracy: 0.7906 - val_auc: 0.8447 - val_loss: 0.4281\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7398 - auc: 0.7984 - loss: 0.4969 - val_accuracy: 0.7977 - val_auc: 0.8519 - val_loss: 0.4181\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7626 - auc: 0.8132 - loss: 0.4634 - val_accuracy: 0.8035 - val_auc: 0.8567 - val_loss: 0.4092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x305bd8ed0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, using the training dataset and validating on the validation dataset.\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fb57147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8035 - auc: 0.8567 - loss: 0.4092\n",
      "{'accuracy': 0.8035087585449219, 'auc': 0.8567060232162476, 'loss': 0.4092298746109009}\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_ds, return_dict=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3011f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_mets_classifier.keras')\n",
    "reloaded_model = tf.keras.models.load_model('my_mets_classifier.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fdaff1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "This particular patient had a 92.4 percent probability of having metabolic syndrome.\n"
     ]
    }
   ],
   "source": [
    "sample = {\n",
    "    'LBDAPBSI': 1.65,\n",
    "    'BMXBMI': 25.0,\n",
    "    'BMXWAIST': 103.6,\n",
    "    'Systolic_BP': 120,\n",
    "    'Diastolic_BP': 80,\n",
    "    'RIDAGEYR': 20,\n",
    "    'RIAGENDR': 1,  # Female (typically 2=female, 1=male in medical datasets)\n",
    "    'DXXSATA': 159.73,\n",
    "    'DXXSATM': 800.18,\n",
    "    'DXXVFATA': 49.66,\n",
    "    'DXXVFATM': 239.42,\n",
    "    'LBXGH': 7.2,\n",
    "    'LBDGLUSI': 5.0,\n",
    "    'LBDHDDSI': 1.4,\n",
    "    'LBXHSCRP': 1.1,\n",
    "    'LBDINSI': 200.82,\n",
    "    'LBDTCSI': 3.78,\n",
    "    'LBDTRSI': 2.0,\n",
    "    'LBDLDLSI': 2.069,\n",
    "    'eLDL_Trig': 0.265,\n",
    "    'Fasting_hrs': 11.75\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "predictions = reloaded_model.predict(input_dict)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This particular patient had a %.1f percent probability \"\n",
    "    \"of having metabolic syndrome.\" % (100 * prob)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
